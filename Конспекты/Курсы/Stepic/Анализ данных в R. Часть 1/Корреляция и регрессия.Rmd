---
title: "Корреляция и регрессия"
author: "Притчин Иван"
date: '14 мая 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(magrittr)
```

Взглянув на диаграмму рассмеивания мы можем составить первое впечатление о том, насколько сильно и в каком направлении связаны две количественные переменные, однако, хотелось бы иметь что-то более конкретное. Не каждый раз смотреть на график а высчитать некоторый коэффициент.
```{r, message=FALSE}
library(ggplot2)
data <- cars
ggplot(cars, aes(speed, dist)) +
  geom_point() + 
  geom_hline(yintercept = mean(cars$dist), color = "blue") + 
  geom_vline(xintercept = mean(cars$speed), color = "red") + 
  scale_x_continuous(name = "Скорость движения") + 
  scale_y_continuous(name = "Длина тормозного пути")
cor.test(data$speed, data$dist)
```
Для каждой точки мы можем вычитать произведение отклонений от среднего:
$$p = (x_i-\overline{X})*(y_i-\overline{Y})$$
Несколько прийти к выводу, что значние $p$ будет положительным в левом нижнем квадрате и в верхнем правом. В оставшихся областях значение будет отрицательным. Сумма произведений отвлонений, деленная на $N-1$ называется ковариацией.
$$cov = \frac{\sum_{1}^n(x_i-\overline{X})*(y_i-\overline{Y})}{N-1}$$
Важно отметить, что значение ковариации может принимать любое значение. Величина определяется значением дисперсии по осям. Для приведения к некоторому стандартному виду, используется коэффициент корреляции:
$$r_{xy}=\frac{cov}{\sigma_x\sigma_y}$$
Такое преобразование гарантирует принадлежность результата промежутку $[-1; 1]$. 

Квадрат коэффициента корреляции называется коэффициентом детерминации, который показывает в каком степени дисперсия одной переменной обусловлена влиянием другой переменной.

##### Условия применения коэффициента корреляции
Условиями применения корреляции является линейность и монотонность данных.

Примеры нелинейной взаимосвязи могут давать корреляцию близкую к нулю. В целом, нечувствителен к нелинейным взаимосвязям. Коэффициент будет равен близок к нулю, но взаимосвязь, очевидно, есть.
```{r}
x = seq(-4, 4, by = 0.1)
make_noise <- function(y, half_range) {
  y + runif(length(y), -half_range, half_range)
}
y1 <- sin(x) %>% make_noise(0.1)
qplot(x, y1) + labs(title=cor(x, y1)) + 
  geom_hline(yintercept = mean(y1), color = "blue") + 
  geom_vline(xintercept = mean(x), color = "red")

```

```{r}
y2 <- x*x %>% make_noise(1)
qplot(x, y2) + labs(title=cor(x, y2)) + geom_hline(yintercept = mean(y2), color = "blue") + 
  geom_vline(xintercept = mean(x), color = "red")
```

Могут попадаться случаи, когда зависимость лучше описывается двумя прямыми, что приводит к тому, что коэффициент корреляции может давать нехорошие результаты.
```{r}
x3 <- c(seq(0, 2, 0.025), seq(2, 4, 0.025))
y3 <- c(-3*x3[1:81], -3*x3[82:162]+16) %>% make_noise(1)
qplot(x3, y3) + labs(title=cor(x3, y3)) + 
  geom_hline(yintercept = mean(y3), color = "blue") + 
  geom_vline(xintercept = mean(x3), color = "red")
```

Важно отметить, что коэффициент очень сильно подвержен выбросам.
```{r}
x <- seq(0, 3, 0.25)
y <- 3*x %>% make_noise(0.2)
qplot(x, y) + labs(title=cor(x, y)) + 
  geom_hline(yintercept = mean(y), color = "blue") + 
  geom_vline(xintercept = mean(x), color = "red") 
```

```{r}
x2 <- c(x, -6, -10)
y2 <- c(y, 20, 50)
qplot(x2, y2) + labs(title=cor(x2, y2)) + 
  geom_hline(yintercept = mean(y2), color = "blue") + 
  geom_vline(xintercept = mean(x2), color = "red") 
```
Вывод: перед применением коэффициента корреляции важно построить график.
Также, было бы неплохо, если переменые x и y были распределены нормально (из-за идеи, которая лежит в основе вычисления). 

##### Корреляция Спирмана
Если есть нарушения условий, можно использовать некоторые непараметрические аналоги. Например, корреляция Спирмана позволяет нам менее чувствительно ощущать последствия выбросов.
Смоделируем работу по нахождению корреляции на языке R
```{r}
# 1. Перевети переменные x и y в разряд ранговых
rank_x2 <- rank(x2)
rank_y2 <- rank(y2)
N <- length(x2)
d2 <- abs(rank_x2 - rank_y2)^2
rs <- 1 - 6*sum(d2)/(N*(N^2 - 1))
data.frame(x2, y2, rank_x2, rank_y2, d2)
```

```{r}
qplot(x2, y2) + labs(title=cor(x2, y2, method = "spearman")) 
```

#### Корреляция и причинно-следственная связь
Стремящаяся к единице по модулю корреляция ничего не говорит нам и причинно-следственной связи.

##TODO
Тут надо обязательно рассказать про ложную корреляцию, пиратов и глобальное потепление
http://www.tylervigen.com/spurious-correlations 
https://www.popmech.ru/science/237232-lozhnye-korrely.. 
https://habr.com/post/236503/

### Регрессия с одной переменной
Линия регрессии показывает нам, как одна переменная объясняет предсказать нам другую переменную.
В общем виде
$$y=\beta_0+\beta_1*x$$
где $\beta_0$ - intercept (определяет точку пересения с осью y), $\beta_1$ - slope (определяет угол наклона). Задача - подобрать эти два параметра, чтобы линия максимально точно отражала взаимосвязь двух количественный переменных
Метод наименьших квадратов - метод нахождения оптимальных параметров линейной регрессии, таких, что сумма квадратов ошибок ($e_i = y_i-\hat{y_1}$) была минимальна. Оцениваются квадраты по причине того, что остатки поглащали друг друга.
$$\beta_1=\frac{sd_y}{sd_x}*r_{xy}$$
$$\beta_0=\overline{Y}-\beta_1*\overline{X}$$
Вопрос о статистической значимости получнной линии регрессии. Если бы не было взаимосвязи между переменными, мы бы получили, что значение $\beta_1 = 0$, $\beta_2=\overline{Y}$. Альтернативная гипотеза: $\beta_0 \neq 0$.
Решение идёт через t-критерий, который говорит следующее: если бы мы многократно повторяли наш эксперимент в ситуации, что у нас никакой взаимосвязи нет, то выборочные значения коэффициентов $\beta_1$ распределились бы нормальным образом вокруг соответствующего значения генеральной совокупности и отклонялись бы то в правую, то в левую сторону от нуля. В таких случаях хорошо использовать t-распределение. Расчитаем t-критерий.  Который заключается с том, насколько выборочное среднее отклонилось от ожидаемого значения в генеральной совокупности, которым является 0, деленное на стандартную ошибку распределения этого коэффициента
$$t=\frac{\beta_1}{se}$$
$df = N-2$
Коэффициент детерминации - доля дисперсии одной переменной, обуславливаемая её взаимосвязью с другой переменной.
$$R^2=1-\frac{SS_{res}}{SS_{total}}$$
$$SS_{res}=\sum_{i=1}^n{e^2}$$
$$SS_{total}=\sum_{i=1}^n{(y-\overline{y})}$$
##### Условия применения линейной регрессии с одним предиктором
Линейная взаимосвязь  
Нормальное распределение остатков
Гомоскедастичность - постоянная изменчивость остатоков на всех уровнях независимой переменной.

##### Применение регрессионного анализа и интерпритация остатков
```{r}
data <- read.csv(file = "datasets/states.csv")
head(data)
str(data)
qplot(x = hs_grad, y = poverty, data = data) + 
  scale_x_continuous(name = "Среднее образование (%)") + 
  scale_y_continuous(name = "Бедность (%)")
fit <- lm(data = data, formula = poverty~hs_grad)
summary(fit)
hist(fit$residuals)
shapiro.test(fit$residuals)
```
В строке с образованием, мы можем заметить, что p-значение меньше 0.05, что говорит о статистически значимой взаимосвязи между двумя переменными. У коэффициента детерминации есть свой p-уровень значимости и значение F-критерия. P-значение для коэффициента детерминации - это некоторая оценка к нашей модели в целом.

#### Задача предсказания в регрессионном анализе

#### Множественная регрессия
Включает несколько предикторов.
Работает аналогично.

Требования:  
- линейная зависимость переменных
- нормальное распределение остатков
- гетероскедастичность
- проверка на мультиколлинеарность - очень сильная взаимосвязь между парой зависимых переменных
- желательно нормальное распределение переменных

```{r}
fit <- lm(data, formula = poverty ~ metro_res + white + hs_grad + female_house)
summary(fit)
```
Однако, стоит отметить, что включение в модель всех параметров далеко не является лучшим решением. Попробуем улучшить модель. Рассмотрим, как переменные коррелируют между собой.
```{r}
corrplot::corrplot(cor(as.data.frame(data[-1])), method = "number")
```
Выполним исключение переменной female_house из-за сильной корреляции со всеми остальными значениями
```{r}
fit <- lm(data, formula = poverty ~ metro_res + white + hs_grad)
summary(fit)
```

Как видим, исправленный R-квадрат стал чуть выше. Однако, при попытке дальнейшего исключения параметров из модели - предсказательная способность не будет становиться лучше.

Вопрос о нелинейности связи.
Рассмотрим следующий случай
```{r}
library(ggplot2)
ggplot() + 
  geom_point(data = mtcars, aes(mpg, hp))
```
Как мы видим, связь не является линейной. Можно выполнить ряд преобразований над данными, чтобы получить линейную взяимосвязь, с которой хорошо будет работать линейная регрессия. Решением может быть тренсформация Тьюки. Основная идея - трансформировать независимую переменную, чтобы ликвидировать нелинейность связи. Мы хотим подобрать такой показатель степени, в который можно возвести каждое из значений независимой переменной так, чтобы итоговая взаимосвязь стала более линейной. Если значение степени меньше нуля - умножаем на -1 для сохранния направления взаимосвязи.
Если лучшим значением является значение в нулевой степени - применяем натуральный логарифм.
```{r, eval=FALSE}
manipulate(
  ggplot() + 
    geom_point(data = mtcars, aes(mpg, hp^st)),
  st = slider(0.5, 2, initial = 1)
)
```

```{r}
fit <- lm(log(mpg) ~ log(hp), mtcars)
summary(fit)
```
Опишем некоторые математические выкладки
$$\log{y_1} =\beta_0+*\beta_1*\log{x_1}$$
$$\log{y_2} =\beta_0+\beta_1*\log{x_2}$$
$$x_2 > x_1$$
$$\log{y_2} - \log{y_1} = \beta_1(\log{x_2}-\log{x_1})$$
$$\frac{y_2}{y_1} = \beta_1\log{\frac{x_2}{x_1}}$$
$$\frac{y_2}{y_1} = (\frac{x_2}{x_1})^{\beta_1}$$
В модели $log{Y}=\beta_0+\beta_1∗X$ коэффициент наклона означает: при единичном изменении переменной $X$, переменная $Y$ в среднем изменяется на $100∗\beta_1$ процентов.

В модели $Y=\beta_0 + \beta_1∗\log{X}$ коэффициент наклона означает:  изменение на 1% по X в среднем приводит к $0.01∗\beta_1$ изменению по переменной $Y$.

#### Трансформация Бокса — Кокса
Трансформация Бокса — Кокса (Box-Cox transformation) — широко используемый метод трансформации данных. В контексте регрессии он обычно используется для трансформации зависимой переменной в случае, если у нас есть ненормальное распределение ошибок и/или нелинейность взаимосвязи, а также в случае гетероскедастичности.

$$y_{new}=\frac{y^p-1}{p} \,\,\,  p\neq 0$$
$$y_{new}=\log(y) \,\,\,  p = 0$$
```{r}
install.packages("lmtest")
```



##### Логистическая регрессия
Позволяет нам исследовать взаимосвязи между зависимой переменной, которая имеет всего лишь две градации и различными предикторами. Позволяет решать целый спектр задач. 